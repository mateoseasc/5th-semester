{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a33eaa",
   "metadata": {},
   "source": [
    "# Práctica Modelo de Lenguaje Probabilista\n",
    "## **Mateo Serrato Ascencio** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2ba9ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRIMERO LEEMOS LOS DATOS \n",
    "\n",
    "def get_text_from_files(path_corpus, path_truth):\n",
    "    tr_text = [] #aqui van los tuits\n",
    "    tr_labels = [] #aqui van las etiquetas\n",
    "\n",
    "    with open(path_corpus, 'r', encoding='utf-8') as f_corpus, open(path_truth, 'r', encoding='utf-8') as f_truth:\n",
    "\n",
    "        for twitt in f_corpus:\n",
    "            tr_text += [twitt]\n",
    "\n",
    "        for label in f_truth:\n",
    "            tr_labels += [label]\n",
    "\n",
    "    return tr_text, tr_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "670afbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_text, tr_labels = get_text_from_files('./mex20_train.txt','mex20_train_labels.txt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b9342035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "class TrigramData:\n",
    "\n",
    "    def __init__(self,vocab_max,tokenizer):\n",
    "        self.vocab_max = vocab_max\n",
    "        self.tokenizer = tokenizer\n",
    "        self.final_vocab = set()\n",
    "        self.sos = '<s>' # start of sentence\n",
    "        self.eos = '</s>' # end of sentence\n",
    "        self.unk = '<unk>' # unknown token\n",
    "    \n",
    "    def fit(self,raw_txt): #raw txt recibe los tweets\n",
    "        freq_dist = FreqDist()\n",
    "        tokenized_corpus = []\n",
    "\n",
    "        for txt in raw_txt:\n",
    "            tokens = self.tokenizer(txt.lower())\n",
    "            tokenized_corpus.append(tokens) #recordar que es una lista de listas de tuit tokenizado\n",
    "            for w in tokens: #Para cada palabra en cada tuit contar en el vocabulario \n",
    "                freq_dist[w] += 1\n",
    "\n",
    "        self.final_vocab = {tok for tok,_ in freq_dist.most_common(self.vocab_max)}\n",
    "        self.final_vocab.update([self.unk,self.sos,self.eos])\n",
    "\n",
    "        transformed_corpus = []\n",
    "        for tokens in tokenized_corpus: #Recordar que recorremos los tokens en cada tuit \n",
    "            transformed_corpus.append(self.transform(tokens)) #Recuerda que tokens es un tuit \n",
    "\n",
    "        return transformed_corpus\n",
    "    \n",
    "    def mask_oov(self,word):\n",
    "        return self.unk if word not in self.final_vocab else word \n",
    "\n",
    "    def add_sos_eos(self, tokens):\n",
    "        return [self.sos,self.sos] + tokens + [self.eos]\n",
    "\n",
    "    def transform(self,tokens):\n",
    "        transformed = [] #tokens transformados \n",
    "        for w in tokens:\n",
    "            transformed.append(self.mask_oov(w)) #mask out of vocabulary word (OOV)\n",
    "        transformed = self.add_sos_eos(transformed) #añade sos y eos \n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aaf61346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>',\n",
       "  '<s>',\n",
       "  '@usuario',\n",
       "  '@usuario',\n",
       "  '@usuario',\n",
       "  'q',\n",
       "  'se',\n",
       "  'puede',\n",
       "  'esperar',\n",
       "  'del',\n",
       "  'maricon',\n",
       "  'de',\n",
       "  'closet',\n",
       "  'de',\n",
       "  'la',\n",
       "  'yañez',\n",
       "  'aun',\n",
       "  'recuerdo',\n",
       "  'esa',\n",
       "  'ves',\n",
       "  'q',\n",
       "  'lo',\n",
       "  'vi',\n",
       "  'en',\n",
       "  'zona',\n",
       "  'rosa',\n",
       "  'viendo',\n",
       "  'quien',\n",
       "  'lo',\n",
       "  'levantada',\n",
       "  '</s>'],\n",
       " ['<s>',\n",
       "  '<s>',\n",
       "  '@usuario',\n",
       "  'la',\n",
       "  'piel',\n",
       "  'nueva',\n",
       "  'siempre',\n",
       "  'arde',\n",
       "  'un',\n",
       "  'poquito',\n",
       "  'los',\n",
       "  'primeros',\n",
       "  'días',\n",
       "  '...',\n",
       "  'y',\n",
       "  'más',\n",
       "  'con',\n",
       "  'este',\n",
       "  'puto',\n",
       "  'clima',\n",
       "  '</s>']]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer().tokenize\n",
    "trigram_data = TrigramData(vocab_max=10000,tokenizer=tokenizer)\n",
    "\n",
    "transformed_corpus = trigram_data.fit(tr_text)\n",
    "final_vocab = trigram_data.final_vocab\n",
    "transformed_corpus[:2]  #muestra los dos primeros tuits tokenizados y transformados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b9667",
   "metadata": {},
   "source": [
    "## BUILDING A TRIGRAM LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95019ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramLanguageModel: #Modelo Interpolado de Unigramas + Bigramas + Trigramas\n",
    "\n",
    "    def __init__(self,lambda1 = .01,lambda2 = .40, lambda3 = .59):\n",
    "        self.lambda1 = lambda1 #Ponderar al Modelo de Lenguaje de Unigramas\n",
    "        self.lambda2 = lambda2 #Ponderar al Modelo de Lenguaje de Bigramas\n",
    "        self.lambda3 = lambda3 #Ponderar al Modelo de Lenguaje de Trigramas \n",
    "    \n",
    "        #Contadores\n",
    "\n",
    "        self.unigram_counts = {} #palabras solitas\n",
    "        self.bigram_counts = {}  #pares de palabras\n",
    "        self.trigram_counts = {} #tripletas de palabras\n",
    "        self.vocab = set()\n",
    "        self.V = 0 #tamaño del vocabulario\n",
    "\n",
    "    def train(self, transformed_corpus, final_vocab):\n",
    "        self.vocab = final_vocab\n",
    "        self.V = len(final_vocab)\n",
    "\n",
    "        for tokens in transformed_corpus: #primero recorremos tuit por tuit \n",
    "            for i,w in enumerate(tokens): #luego para cada tuit recorremos palabra por palabra \n",
    "                #Unigramas\n",
    "                self.unigram_counts[w] = self.unigram_counts.get(w,0) + 1\n",
    "                \n",
    "                #Bigramas\n",
    "                if i >= 1:\n",
    "                   w_prev = tokens[i-1]\n",
    "                   self.bigram_counts[w_prev,w] = self.bigram_counts.get((w_prev,w),0) + 1\n",
    "\n",
    "                #Trigramas\n",
    "                if i >= 2:\n",
    "                  w_prev2 = tokens[i-2]\n",
    "                  self.trigram_counts[w_prev2,w_prev,w] = self.trigram_counts.get((w_prev2,w_prev,w),0) + 1\n",
    "            self.total_tokens = sum(self.unigram_counts.values())\n",
    "\n",
    "    def mask_oov(self,word):\n",
    "        return \"<unk>\" if word not in self.vocab else word\n",
    "\n",
    "    def unigram_prob(self,w):\n",
    "        numerator = self.unigram_counts.get(self.mask_oov(w),0) + 1\n",
    "        denominator = self.total_tokens + self.V\n",
    "    \n",
    "\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def bigram_prob(self,w_prev,w): # P(w|w_prev)\n",
    "        w_prev = self.mask_oov(w_prev)\n",
    "        w = self.mask_oov(w)\n",
    "        numerator = self.bigram_counts.get((w_prev,w),0) + 1\n",
    "        denominator = self.unigram_counts.get((w_prev),0) + self.V\n",
    "        \n",
    "        return numerator / denominator\n",
    "    \n",
    "    def trigram_prob(self,w_prev2,w_prev,w): #P(w|w_prev2,w_prev)\n",
    "        w_prev2 = self.mask_oov(w_prev2)\n",
    "        w_prev = self.mask_oov(w_prev)\n",
    "        w = self.mask_oov(w)\n",
    "        numerator = self.trigram_counts.get((w_prev2,w_prev,w),0) + 1 #Cuantas veces aparecio la secuencia completa\n",
    "        denominator = self.bigram_counts.get((w_prev2,w_prev),0) + self.V # Cuantas veces aparecio el contexto (la secuencia previa a ww)\n",
    "\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def checar_prob(self):\n",
    "        print(sum(self.unigram_prob(w) for w in self.vocab))\n",
    "        print(sum(self.bigram_prob('gato',w) for w in self.vocab))\n",
    "        print(sum(self.trigram_prob('hola','como',w) for w in self.vocab))\n",
    "\n",
    "    def top_next_words(self,w_prev2,w_prev,top_k = 5):\n",
    "        candidates = []\n",
    "        for cand in self.vocab:\n",
    "            p_cand = self.probability_of_word(w_prev2,w_prev,cand)\n",
    "            candidates.append((cand,p_cand))\n",
    "        candidates = sorted(candidates,key=lambda x:x[1],reverse=True)\n",
    "        return candidates[:top_k]\n",
    "\n",
    "    def probability_of_word(self,w_prev2,w_prev,w):\n",
    "        p3 = self.trigram_prob(w_prev2,w_prev,w)\n",
    "        p2 = self.bigram_prob(w_prev,w)\n",
    "        p1 = self.unigram_prob(w)\n",
    "\n",
    "        lambda1 = self.lambda1 \n",
    "        lambda2 = self.lambda2 \n",
    "        lambda3 = self.lambda3 \n",
    "\n",
    "        P_interpolada = p1*lambda1 + p2*lambda2 + p3* lambda3\n",
    "        return P_interpolada\n",
    "    \n",
    "    def sequence_probability(self,sequence):\n",
    "        import math \n",
    "        log_prob = 0\n",
    "        for i in range(2,len(sequence)):\n",
    "            w_prev2 = sequence[i-2]\n",
    "            w_prev = sequence[i-1]\n",
    "            w = sequence[i]\n",
    "\n",
    "            p = self.probability_of_word(w_prev2,w_prev,w)\n",
    "            log_prob += math.log(p)\n",
    "\n",
    "        return math.exp(log_prob)\n",
    "\n",
    "\n",
    "    def rank_permutations(self,tokens,top_k=5,bottom_k=5):\n",
    "        from itertools import permutations\n",
    "        perms = set(permutations(tokens))\n",
    "        scores = []\n",
    "        for p in perms:\n",
    "            prob = self.sequence_probability(list(p))\n",
    "            scores.append((\" \".join(p),prob))\n",
    "        scores.sort(key = lambda x: x[1],reverse=True)\n",
    "\n",
    "        best = scores[:top_k]\n",
    "        worst = scores[-bottom_k:]\n",
    "        return best, worst\n",
    "    \n",
    "    def generate_from_tokens_as_chatgpt(self, first_word,second_word,max_length = 20,delay = 0.5):\n",
    "        if first_word not in self.vocab:\n",
    "            first_word = '<unk>'        \n",
    "        if second_word not in self.vocab:\n",
    "            second_word = '<unk>'   \n",
    "        generated = [first_word,second_word]\n",
    "\n",
    "        print(first_word,second_word,end=\" \",flush=True) \n",
    "\n",
    "        for _ in range(max_length):\n",
    "            w_prev2 = generated[-2]\n",
    "            w_prev = generated[-1]\n",
    "\n",
    "            dist = []\n",
    "            total_p = 0.0\n",
    "            for w in self.vocab:\n",
    "                p = self.probability_of_word(w_prev2,w_prev,w)\n",
    "                dist.append((w,p))\n",
    "                total_p += p\n",
    "            if total_p == 0.0:\n",
    "                break\n",
    "\n",
    "            import random \n",
    "            import time\n",
    "            import sys\n",
    "\n",
    "            r = random.random() * total_p\n",
    "            cumulative = 0.0\n",
    "            chosen = None\n",
    "            for w,p in dist:\n",
    "                cumulative += p\n",
    "                if cumulative >= r:\n",
    "                    chosen = w\n",
    "                    break\n",
    "            if chosen is None or chosen == '</s>':\n",
    "                break\n",
    "\n",
    "            generated.append(chosen)\n",
    "            print(chosen,end= \" \",flush = True)\n",
    "            time.sleep(delay)\n",
    "\n",
    "        print()\n",
    "        return \" \".join(generated)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608cdcf3",
   "metadata": {},
   "source": [
    "# Probando el Modelo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "78d54de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999782\n",
      "0.9999999999998422\n",
      "1.0000000000000644\n"
     ]
    }
   ],
   "source": [
    "trigram_lm = TrigramLanguageModel()\n",
    "trigram_lm.train(transformed_corpus,final_vocab)\n",
    "trigram_lm.checar_prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1b7259f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0285%\n"
     ]
    }
   ],
   "source": [
    "w_prev2,w_prev,w = 'vete',\"a\",'el'\n",
    "p_w = trigram_lm.probability_of_word(w_prev2,w_prev,w)\n",
    "print(f\"{p_w * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "926ed641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('verga', 0.015603964747744226),\n",
       " ('madre', 0.005336759556709187),\n",
       " ('<unk>', 0.004533091183708973),\n",
       " ('chingada', 0.0024771255985542757),\n",
       " ('gente', 0.002447574091646817)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5 = (trigram_lm.top_next_words('a','la',top_k=5))\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "02e6055c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('verga', 0.005137069815879167),\n",
       " ('madre', 0.001970339559950493),\n",
       " ('<s>', 0.0009542736274069821),\n",
       " ('</s>', 0.0005262467033943309),\n",
       " ('que', 0.0004881400079337845)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 ejemplos de  top_5\n",
    "top_5 = (trigram_lm.top_next_words('hijo','de',top_k=5))\n",
    "top_5\n",
    "\n",
    "top_5 = (trigram_lm.top_next_words('chinga','tu',top_k=5))\n",
    "top_5\n",
    "\n",
    "top_5 = (trigram_lm.top_next_words('soy','la',top_k=5))\n",
    "top_5\n",
    "\n",
    "top_5 = (trigram_lm.top_next_words('sois','unos',top_k=5))\n",
    "top_5\n",
    "\n",
    "top_5 = (trigram_lm.top_next_words('me','vale',top_k=5))\n",
    "top_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bebc66a",
   "metadata": {},
   "source": [
    "# Evaluación Cualitativa con los modelos de Lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8a0860ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = [\"sino\",\"gano\",\"me\",\"voy\",\"a\",\"la\",\"chingada\",\",\"]\n",
    "\n",
    "\n",
    "top_5,bottom_5 =trigram_lm.rank_permutations(test_tokens,top_k=5,bottom_k=5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "84141fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('gano sino , me voy a la chingada', 1.4556256451760978e-16),\n",
       "  ('sino gano , me voy a la chingada', 1.304113035182973e-16),\n",
       "  ('gano , me voy a la chingada sino', 3.7665331039448914e-17),\n",
       "  ('sino , me voy a la chingada gano', 3.744980003570942e-17),\n",
       "  ('gano me voy a la chingada , sino', 3.225307430667454e-17)],\n",
       " [(', a sino me chingada voy la gano', 5.631051079606779e-24),\n",
       "  ('a , gano voy la sino me chingada', 5.532510787112802e-24),\n",
       "  ('a , gano la sino voy me chingada', 5.529217166054525e-24),\n",
       "  ('a , gano la sino me chingada voy', 5.5278547255835275e-24),\n",
       "  ('a , gano me chingada voy la sino', 5.519840317620859e-24)])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_lm.sequence_probability(test_tokens)\n",
    "top_5,bottom_5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
